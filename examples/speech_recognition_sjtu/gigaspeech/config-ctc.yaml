# @package _group_ 

common:
  log_interval: 100
  log_format: json
  tensorboard_logdir: tb
  wandb_project: fairseq-gigaspeech-ctc
  seed: 1
  fp16: true

distributed_training:
  distributed_world_size: 1
  ddp_backend: pytorch_ddp

dataset:
  num_workers: 4
  max_tokens: 90000
  skip_invalid_size_inputs_valid_test: true
  train_subset: train-m
  valid_subset: dev-tidy
  validate_interval: 1
  validate_interval_updates: 0
  max_tokens_valid: 60000

optimization:
  max_update: 150000
  clip_norm: 5.0
  sentence_avg: false
  update_freq: [1]
  lr: [0.0005]

optimizer:
  _name: adam
  adam_betas: (0.9,0.999)
  adam_eps: 1.0e-08
  # weight_decay: 0.01

lr_scheduler:
  _name: linear_decay
  # _name: inverse_sqrt
  warmup_updates: 15000

model:
  _name: sr_ctc
  encoder_type: conformer
  # encoder_type: transformer
  encoder_embed_dim: 512
  encoder_ffn_embed_dim: 2048
  encoder_attention_heads: 8
  encoder_layers: 12
  outproj_dim: 512


criterion:
  _name: ctc_loss
  sentence_avg: true
  post_process: sentencepiece

checkpoint:
  save_dir: /home/chenxie95/github/fairseq/outputs/asr_gigaspeech_ctc.withspecaug-lineardecay
  save_interval_updates: 0
  keep_interval_updates: -1
  no_epoch_checkpoints: false

task:
  _name: speech_recognition
  data: /mnt/data/GigaSpeech/tempdir
  config_yaml: config-specaug.yaml

hydra:
    run:
        dir: ${checkpoint.save_dir}
