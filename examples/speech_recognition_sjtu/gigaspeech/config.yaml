# @package _group_ 

common:
  log_interval: 100
  log_format: json
  tensorboard_logdir: tb
  seed: 1
  fp16: true

distributed_training:
  distributed_world_size: 1
  ddp_backend: pytorch_ddp

dataset:
  num_workers: 4
  max_tokens: 40000
  skip_invalid_size_inputs_valid_test: true
  train_subset: train-m
  valid_subset: dev
  validate_interval: 1
  validate_interval_updates: 0
  max_tokens_valid: 40000

optimization:
  max_update: 10000
  clip_norm: 10.0
  sentence_avg: false
  update_freq: [8]
  lr: [0.002]

optimizer:
  _name: adam
  adam_betas: (0.9,0.999)
  adam_eps: 1.0e-08
  weight_decay: 0.01

lr_scheduler:
  _name: inverse_sqrt
  warmup_updates: 1000

model:
  _name: sr_transformer
  share_decoder_input_output_embed: true

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.1
  report_accuracy: true
  ignore_prefix_size: 0
  sentence_avg: false

checkpoint:
  save_dir: /home/chenxie95/github/fairseq/outputs/asr_gigaspeech
  save_interval_updates: 0
  keep_interval_updates: -1
  no_epoch_checkpoints: false

task:
  _name: speech_recognition
  data: /mnt/data/GigaSpeech/tempdir
  config_yaml: config.yaml

